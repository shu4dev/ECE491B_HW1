============================= test session starts ==============================
platform darwin -- Python 3.10.16, pytest-8.3.4, pluggy-1.5.0
rootdir: /Users/brianshu/workspace/ECE491B_HW1
configfile: pytest.ini
collected 23 items

tests/test_tokenizer.py::test_roundtrip_empty PASSED                     [  4%]
tests/test_tokenizer.py::test_empty_matches_tiktoken PASSED              [  8%]
tests/test_tokenizer.py::test_roundtrip_single_character PASSED          [ 13%]
tests/test_tokenizer.py::test_single_character_matches_tiktoken PASSED   [ 17%]
tests/test_tokenizer.py::test_roundtrip_single_unicode_character PASSED  [ 21%]
tests/test_tokenizer.py::test_single_unicode_character_matches_tiktoken PASSED [ 26%]
tests/test_tokenizer.py::test_roundtrip_ascii_string PASSED              [ 30%]
tests/test_tokenizer.py::test_ascii_string_matches_tiktoken PASSED       [ 34%]
tests/test_tokenizer.py::test_roundtrip_unicode_string PASSED            [ 39%]
tests/test_tokenizer.py::test_unicode_string_matches_tiktoken PASSED     [ 43%]
tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens FAILED [ 47%]
tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken FAILED [ 52%]
tests/test_tokenizer.py::test_overlapping_special_tokens FAILED          [ 56%]
tests/test_tokenizer.py::test_address_roundtrip PASSED                   [ 60%]
tests/test_tokenizer.py::test_address_matches_tiktoken PASSED            [ 65%]
tests/test_tokenizer.py::test_german_roundtrip PASSED                    [ 69%]
tests/test_tokenizer.py::test_german_matches_tiktoken PASSED             [ 73%]
tests/test_tokenizer.py::test_tinystories_sample_roundtrip PASSED        [ 78%]
tests/test_tokenizer.py::test_tinystories_matches_tiktoken PASSED        [ 82%]
tests/test_tokenizer.py::test_encode_iterable_tinystories_sample_roundtrip PASSED [ 86%]
tests/test_tokenizer.py::test_encode_iterable_tinystories_matches_tiktoken PASSED [ 91%]
tests/test_tokenizer.py::test_encode_iterable_memory_usage SKIPPED (...) [ 95%]
tests/test_tokenizer.py::test_encode_memory_usage SKIPPED (rlimit su...) [100%]

=================================== FAILURES ===================================
______________ test_roundtrip_unicode_string_with_special_tokens _______________

    def test_roundtrip_unicode_string_with_special_tokens():
        tokenizer = get_tokenizer_from_vocab_merges_path(
            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
        )
        test_string = "HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>"
        encoded_ids = tokenizer.encode(test_string)
        tokenized_string = [tokenizer.decode([x]) for x in encoded_ids]
        # Ensure the special <|endoftext|> token is preserved
>       assert tokenized_string.count("<|endoftext|>") == 3
E       AssertionError: assert 1 == 3
E        +  where 1 = <built-in method count of list object at 0x129976040>('<|endoftext|>')
E        +    where <built-in method count of list object at 0x129976040> = ['H', 'Ã©', 'll', 'ï¿½', 'ï¿½', ' h', ...].count

tests/test_tokenizer.py:232: AssertionError
___________ test_unicode_string_with_special_tokens_matches_tiktoken ___________

    def test_unicode_string_with_special_tokens_matches_tiktoken():
        reference_tokenizer = tiktoken.get_encoding("gpt2")
        tokenizer = get_tokenizer_from_vocab_merges_path(
            vocab_path=VOCAB_PATH, merges_path=MERGES_PATH, special_tokens=["<|endoftext|>"]
        )
        test_string = "HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ðŸ™ƒ<|endoftext|>"
    
        reference_ids = reference_tokenizer.encode(
            test_string, allowed_special={"<|endoftext|>"}
        )
        ids = tokenizer.encode(test_string)
>       assert ids == reference_ids
E       AssertionError: assert [39, 2634, 29...110, 289, ...] == [39, 2634, 29...110, 289, ...]
E         
E         At index 8 diff: 1279 != 220
E         Left contains 10 more items, first extra item: 91
E         
E         Full diff:
E           [
E               39,...
E         
E         ...Full output truncated (34 lines hidden), use '-vv' to show

tests/test_tokenizer.py:249: AssertionError
_______________________ test_overlapping_special_tokens ________________________

    def test_overlapping_special_tokens():
        tokenizer = get_tokenizer_from_vocab_merges_path(
            vocab_path=VOCAB_PATH,
            merges_path=MERGES_PATH,
            special_tokens=["<|endoftext|>", "<|endoftext|><|endoftext|>"],
        )
        test_string = "Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>"
    
        ids = tokenizer.encode(test_string)
        tokenized_string = [tokenizer.decode([x]) for x in ids]
        # Ensure the double <|endoftext|><|endoftext|> is preserved as a single token
        assert tokenized_string.count("<|endoftext|>") == 1
>       assert tokenized_string.count("<|endoftext|><|endoftext|>") == 1
E       AssertionError: assert 0 == 1
E        +  where 0 = <built-in method count of list object at 0x12a543540>('<|endoftext|><|endoftext|>')
E        +    where <built-in method count of list object at 0x12a543540> = ['Hello', ',', ' how', ' <', '|', 'end', ...].count

tests/test_tokenizer.py:267: AssertionError
=========================== short test summary info ============================
FAILED tests/test_tokenizer.py::test_roundtrip_unicode_string_with_special_tokens
FAILED tests/test_tokenizer.py::test_unicode_string_with_special_tokens_matches_tiktoken
FAILED tests/test_tokenizer.py::test_overlapping_special_tokens - AssertionEr...
=================== 3 failed, 18 passed, 2 skipped in 6.90s ====================
